{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b69a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor, Trainer, TrainingArguments, Wav2Vec2ForSequenceClassification\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn as nn\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "\n",
    "csv_path = '/kaggle/input/meld-data/MELD_complete_dataset.csv'\n",
    "df = pd.read_csv(csv_path, encoding='utf-8', on_bad_lines='skip')\n",
    "\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, row in enumerate(data_dict[:3]): \n",
    "    print(f\"\\n Record {i+1}:\")\n",
    "    for key, value in row.items():\n",
    "        print(f\"{key:<15}: {value}\")\n",
    "\n",
    "\n",
    "\n",
    "df = df.drop(['Speaker','Sentiment', 'Dialogue_ID', 'Utterance_ID', 'Season',\n",
    "              'StartTime', 'EndTime','Episode','audio_path'], axis=1, errors='ignore')\n",
    "\n",
    "data_dict = df.to_dict(orient='records')\n",
    "\n",
    "\n",
    "row = data_dict[0]\n",
    "for key, value in row.items():\n",
    "    print(f\"{key:<15}: {value}\")\n",
    "\n",
    "\n",
    "print(df['Emotion'].value_counts())\n",
    "\n",
    "\n",
    "texts = df['Utterance'].astype(str).tolist()\n",
    "labels = df['Emotion'].astype(str).tolist()\n",
    "\n",
    "tokenized = [t.lower().split() for t in texts]\n",
    "word_counts = Counter(w for sent in tokenized for w in sent)\n",
    "vocab = {w: i+2 for i, (w, _) in enumerate(word_counts.items())}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "\n",
    "\n",
    "glove_path = \"/kaggle/input/embeddings/glove.6B.100d.txt\" \n",
    "embedding_dim = 100  \n",
    "\n",
    "embeddings_index = {}\n",
    "with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# --- Create embedding matrix ---\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "for word, i in vocab.items():\n",
    "    vector = embeddings_index.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[i] = vector\n",
    "\n",
    "\n",
    "\n",
    "encoded_texts = [[vocab.get(w, 1) for w in sent] for sent in tokenized]\n",
    "\n",
    "label_to_idx = {l: i for i, l in enumerate(sorted(set(labels)))}\n",
    "idx_to_label = {i: l for l, i in label_to_idx.items()}\n",
    "encoded_labels = [label_to_idx[l] for l in labels]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(encoded_texts, encoded_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "X_train_tensors = [torch.tensor(x, dtype=torch.long) for x in X_train]\n",
    "X_test_tensors = [torch.tensor(x, dtype=torch.long) for x in X_test]\n",
    "\n",
    "X_train_padded = pad_sequence(X_train_tensors, batch_first=True, padding_value=0)\n",
    "X_test_padded = pad_sequence(X_test_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "print(\"Train shape:\", X_train_padded.shape)\n",
    "print(\"Test shape:\", X_test_padded.shape)\n",
    "\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.conv = nn.Conv1d(embed_dim, 128, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lstm = nn.LSTM(128, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.relu(self.conv(x))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        h = torch.cat((h[-2], h[-1]), dim=1)\n",
    "        return self.fc(h)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "num_classes = len(label_to_idx)\n",
    "model = CNN_LSTM(vocab_size, embed_dim=100, hidden_dim=128, num_classes=num_classes)\n",
    "\n",
    "\n",
    "\n",
    "model.embedding.weight.data.copy_(torch.tensor(embedding_matrix))\n",
    "model.embedding.weight.requires_grad = True  # allow fine-tuning\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f2a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "val_test_avg=0\n",
    "val_train_avg=0\n",
    "f1_avg=0\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    \n",
    "    for i in range(0, len(X_train_padded), batch_size):\n",
    "        X_batch = X_train_padded[i:i+batch_size]\n",
    "        y_batch = y_train_tensor[i:i+batch_size]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        preds = model(X_test_padded).argmax(dim=1).cpu().numpy()\n",
    "        y_true = y_test_tensor.cpu().numpy()\n",
    "\n",
    "        preds_train = model(X_train_padded).argmax(dim=1).cpu().numpy()\n",
    "        y_true_train = y_train_tensor.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(y_true, preds)\n",
    "    acc2 = accuracy_score(y_true_train, preds_train)\n",
    "    f1 = f1_score(y_true, preds, average='weighted')\n",
    "    val_test_avg+=acc\n",
    "    val_train_avg+=acc2\n",
    "    f1_avg+=f1\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{epochs}]  |  Loss: {total_loss:.4f}  |  Val Acc: {acc*100:.2f}% | Val Acc2: {acc2*100:.2f}%|  F1: {f1:.4f}\")\n",
    "\n",
    "print(f\"Validation test accuracy: {(val_test_avg/epochs)*100:.2f}% | Validation train Accuracy: {(val_train_avg/epochs)*100:.2f}%|  F1: {(f1_avg/epochs):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370623e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_emotion(sentence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        tokens = sentence.lower().split()\n",
    "        encoded = [vocab.get(w, 1) for w in tokens]\n",
    "\n",
    "        # Convert to tensor (1, seq_len)\n",
    "        x_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "        # Directly feed to model (no extra pad_sequence)\n",
    "        output = model(x_tensor)\n",
    "        pred = torch.argmax(output, dim=1).item()\n",
    "\n",
    "        emotion = idx_to_label[pred]\n",
    "        return emotion\n",
    "\n",
    "# Example usage\n",
    "test_sentence = \"I am really Happy today\"\n",
    "predicted_emotion = predict_emotion(test_sentence)\n",
    "print(f\"Sentence: {test_sentence}\")\n",
    "print(f\"Predicted Emotion: {predicted_emotion}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
